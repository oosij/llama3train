{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60c14951-587c-4dbe-a203-3c44fe6293e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin C:\\Users\\any\\anaconda3\\envs\\oosij\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda118.dll\n"
     ]
    }
   ],
   "source": [
    "# https://medium.com/@miloszivic99/finetuning-large-language-models-customize-llama-3-8b-for-your-needs-bfe0f43cd239\n",
    "import re \n",
    "import os\n",
    "import os.path as osp\n",
    "import torch\n",
    "import pandas\n",
    "import pandas as pd\n",
    "import json\n",
    "from typing import Union\n",
    "from typing import List\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset, DatasetDict\n",
    "from datasets import concatenate_datasets\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9625fe1e-d3e0-4688-be9c-5e399f80d169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Llama-3-Open-Ko-8B-Instruct-preview_train_v5'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_path = './data/classification/ukairia777/finance_data.csv' # 분류 .csv\n",
    "s_path = './data/summarization/aihub_news' # 요약 폴더내.json 들\n",
    "template_path = './templates/multi.json' # 템플릿  .json\n",
    "dataset_name = \"Smoked-Salmon-s/empathetic_dialogues_ko\" # 싱글/멀티 대화형 데이터셋  허깅페이스 :\n",
    "\n",
    "model_name = \"beomi/Llama-3-Open-Ko-8B-Instruct-preview\"\n",
    "\n",
    "name = model_name.split('/')[1]\n",
    "new_model = name + \"_train_v5\" \n",
    "new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15edceda-69ab-4d8d-921a-4ea05c29a00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_classification_dataset(c_path):\n",
    "  # Read the CSV file into a pandas DataFrame\n",
    "  df = pd.read_csv(c_path)\n",
    "  # Replace values in the 'labels' column\n",
    "  df['labels'] = df['labels'].replace({'neutral': '중립', 'negative': '부정', 'positive': '긍정'})\n",
    "\n",
    "  ## 허깅페이스 데이터셋화\n",
    "\n",
    "  # Hugging Face의 dataset 포맷으로 변환\n",
    "  huggingface_dataset = Dataset.from_pandas(df.rename(columns={'labels': 'output', 'kor_sentence': 'input'}))\n",
    "\n",
    "  # instruction 추가 및 컬럼 순서 변경\n",
    "  huggingface_dataset = huggingface_dataset.map(\n",
    "      lambda example: {\"instruction\": '밑의 내용의 감성을 분석하고, 그것이 긍정, 중립, 아니면 부정인지 결정하고 대답해주세요. 해당 감정 레이블은 \"긍정\", \"중립\" 또는 \"부정\"입니다.',\n",
    "                       \"input\": example[\"input\"],\n",
    "                       \"output\": example[\"output\"]\n",
    "                       },\n",
    "      remove_columns=['sentence']\n",
    "  )\n",
    "\n",
    "  # 필드와 값을 추가\n",
    "  sentiment_dataset = huggingface_dataset.map(\n",
    "      lambda example: {'instruction': example['instruction'], 'input': example['input'], 'output': example['output'], 'source': 'github/ukairia777', 'type': 'task_classification'})\n",
    "\n",
    "  return sentiment_dataset\n",
    "\n",
    "def task_summarization_dataset(s_path):\n",
    "  folder_path  = s_path\n",
    "\n",
    "  # Initialize an empty list to store data\n",
    "  data_list = []\n",
    "\n",
    "  # Get a list of all files in the folder\n",
    "  file_list = [f for f in os.listdir(folder_path) if f.endswith('.json')]\n",
    "\n",
    "  # Loop through each JSON file and extract 'passage' and 'summary2'\n",
    "  for file_name in file_list:\n",
    "      file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "      # Read the JSON file\n",
    "      with open(file_path, 'r', encoding='utf-8') as file:\n",
    "          data = json.load(file)\n",
    "\n",
    "      # Extract 'passage' and 'summary2' fields\n",
    "      passage = data.get('Meta(Refine)', {}).get('passage', '')\n",
    "      summary2 = data.get('Annotation', {}).get('summary2', '')\n",
    "\n",
    "      #if len(summary2.split('. ')) <= 1: # 원인 불명 ?  그냥 문장 길이 미만인 거만 짜르도록\n",
    "      if len(summary2) <= 100: # 문장 길이가 100 이하인 것은 버리기\n",
    "        continue\n",
    "\n",
    "      # Append data to the list\n",
    "      data_list.append({'file_name': file_name, 'body': passage, 'summary': summary2})\n",
    "\n",
    "  # Convert the list of dictionaries to a pandas DataFrame\n",
    "  df = pd.DataFrame(data_list)\n",
    "\n",
    "  # Hugging Face의 dataset으로 변환\n",
    "  hf_dataset = Dataset.from_pandas(df)\n",
    "\n",
    "  # 필요한 형태로 변환\n",
    "  hf_dataset = hf_dataset.map(\n",
    "      lambda example: {'instruction': '밑의 내용을 요약해주세요.',\n",
    "                       'input': example['body'],\n",
    "                       'output': example['summary']\n",
    "                       },\n",
    "      remove_columns=['file_name','body','summary']\n",
    "                     )\n",
    "  # 필드와 값을 추가\n",
    "  summary_dataset = hf_dataset.map(\n",
    "      lambda example: {'instruction': example['instruction'], 'input': example['input'], 'output': example['output'], 'source': 'aihub', 'type': 'task_summarization'})\n",
    "  return summary_dataset\n",
    "\n",
    "\n",
    "def task_combined_dataset(c_dataset, s_dataset):\n",
    "  # 두 데이터셋 합치기\n",
    "  combined_dataset = concatenate_datasets([c_dataset, s_dataset])\n",
    "  # 데이터셋 섞기\n",
    "  combined_dataset = combined_dataset.shuffle(seed=42)  # seed 값은 원하는 값으로 변경 가능\n",
    "  # instruction과 input을 합쳐서 새로운 instruction에 값을 주기\n",
    "  combined_dataset = combined_dataset.map(lambda example: {'new_instruction': example['instruction'].replace('#','').strip() + '\\n\\n' + example['input'].rstrip(), 'output': example['output'], 'source': example['source'], 'type': example['type']})\n",
    "\n",
    "  # 기존의 instruction과 input 삭제\n",
    "  combined_dataset = combined_dataset.remove_columns(['instruction', 'input'])\n",
    "  # 'new_instruction' 필드의 이름을 'instruction'으로 변경\n",
    "  combined_dataset = combined_dataset.rename_column('new_instruction', 'instruction')\n",
    "\n",
    "  return combined_dataset\n",
    "\n",
    "# 챗 템플릿 v004 : by llama 3 \n",
    "def process_dataset(dataset):\n",
    "    #your_system_message = \"친절한 챗봇으로서 상대방의 요청에 최대한 자세하고 친절하게 답하자. 모든 대답은 한국어(Korean)으로 대답해줘.\"\n",
    "    your_system_message = \"당신은 업무적으로 도움을 주고 공감 능력이 있는 AI 봇 픽시입니다. 다음 대화의 흐름을 보고 상대방의 요구에 맞는 답변을 해주세요.\"\n",
    "    \n",
    "    begin_token = '<|begin_of_text|>'\n",
    "    bos_token = '<|eot_id|>'\n",
    "    sep_start_token = '<|start_header_id|>'\n",
    "    sep_end_token = '<|end_header_id|>'\n",
    "\n",
    "    assistant_full_token = bos_token + sep_start_token + 'assistant' + sep_end_token\n",
    "    user_full_token = bos_token + sep_start_token + 'user' + sep_end_token\n",
    "    \n",
    "    assistant_token =  sep_start_token + 'assistant' + sep_end_token\n",
    "    user_token = sep_start_token + 'user' + sep_end_token\n",
    "    system_token =  sep_start_token + 'system' + sep_end_token\n",
    "\n",
    "\n",
    "    def format_single(instruction, output):\n",
    "         return system_token +f\"\\n\\n{your_system_message}\" + user_full_token + f\"\\n\\n{instruction}\" + assistant_full_token + f\"\\n\\n{output}<|eot_id|>\"\n",
    "\n",
    "    def format_multi(instruction, output):\n",
    "        instruction_parts = instruction.split('\\n')\n",
    "        messages = []\n",
    "        for part in range(len(instruction_parts)):\n",
    "            inst_part = instruction_parts[part]\n",
    "            if inst_part.startswith('질문:'):\n",
    "                messages.append(user_token + '\\n\\n' + f\"{inst_part[len('질문: '):].strip()}\"+ bos_token)\n",
    "            elif inst_part.startswith('답변:'):\n",
    "                messages.append(assistant_token + '\\n\\n'f\"{inst_part[len('답변: '):].strip()}\"+ bos_token)\n",
    "\n",
    "        segments = []\n",
    "\n",
    "        for m in range(len(messages)):\n",
    "            if m == 0:\n",
    "                # 시스템 메시지를 첫 번째 세그먼트에만 포함시키기\n",
    "                system_msg = system_token +f\"\\n\\n{your_system_message}\"  + bos_token +  f\"{messages[0]}\"\n",
    "                segments.append(system_msg)\n",
    "            else:\n",
    "                inst_msg = messages[m]\n",
    "                segments.append(inst_msg)\n",
    "\n",
    "        seg_response =  assistant_token + f'\\n\\n{output}' + '<|eot_id|>'\n",
    "        segments.append(seg_response)\n",
    "\n",
    "        return ''.join(segments)\n",
    "\n",
    "    # Apply formatting to each row and construct the new dataset structure\n",
    "    new_columns = {key: [] for key in dataset.column_names + ['text']}\n",
    "    for row in dataset:\n",
    "        for key in dataset.column_names:\n",
    "            new_columns[key].append(row[key])\n",
    "        \n",
    "        if 'multi' in row['type']:\n",
    "            in_text = format_multi(row['instruction'], row['output'])\n",
    "            in_text = remove_emojis(in_text)\n",
    "            new_columns['text'].append(in_text)\n",
    "        else:\n",
    "            in_text = format_single(row['instruction'], row['output'])\n",
    "            in_text = remove_emojis(in_text)\n",
    "            new_columns['text'].append(in_text)\n",
    "\n",
    "    return Dataset.from_dict(new_columns)\n",
    "\n",
    "\n",
    "def remove_emojis(text):\n",
    "    # 모든 이모지에 대응하는 유니코드 범위를 좀 더 세밀하게 지정\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               \"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
    "                               \"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "                               \"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "                               \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "                               \"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "                               \"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "                               #\"\\U000024C2-\\U0001F251\"  # Enclosed characters\n",
    "                               \"\\U00002500-\\U000026FF\"  # Miscellaneous Symbols\n",
    "                               \"\\U00002B50-\\U00002B55\"  # Additional emoticons\n",
    "                               \"\\U0001F000-\\U0001F02F\"  # Mahjong tiles\n",
    "                               \"\\U0001F0A0-\\U0001F0FF\"  # Playing cards\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c25b8e5-0875-42ea-9093-bea31fabafd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset =  load_dataset(dataset_name) # train_args.data_path\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d878351-3418-479b-a75f-c0be08a72074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1316e3e521814bbdafcf8e340d29f7b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4846 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9190516c9354c5882bd86733f9b4b87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4846 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5510a5c604741e1a03cb573a057d593",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9836 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b00c590fe10140fdb93ed3034f5aed0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9836 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "c_dataset= task_classification_dataset(c_path)\n",
    "s_dataset = task_summarization_dataset(s_path)\n",
    "chat_dataset = process_dataset(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28e36975-6db2-4fee-98a3-0a9ac00ca777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "당신은 업무적으로 도움을 주고 공감 능력이 있는 AI 봇 픽시입니다. 다음 대화의 흐름을 보고 상대방의 요구에 맞는 답변을 해주세요.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "결혼식 준비로 인해 스트레스 받고 있어.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "결혼식 준비로 인해 스트레스를 받고 계시군요. 아름다운 순간을 위한 준비는 때때로 힘들 수 있어요. 그래도 괜찮아요, 여러분의 특별한 날을 위한 것이니까요. 어떤 부분이 가장 도움이 필요하신가요?<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(chat_dataset['text'][622])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0eead38-8a96-4081-b533-24343d1a08c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddadd73-fc8f-4f47-9b09-472bd30c49d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "543b22c5-7e3c-4960-b1b7-964480d7ae03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape: 4063\n",
      "Test set shape: 783\n",
      "프로그램 실행 시간:  206.30734992027283 초\n",
      "Train set shape: 4492\n",
      "Test set shape: 5344\n",
      "Train set shape: 20562\n",
      "Test set shape: 6100\n",
      "프로그램 실행 시간:  0.2540562152862549 초\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# 'output' 값에 대한 각 인덱스 구하기\n",
    "positive_indices = [idx for idx, label in enumerate(c_dataset['output']) if label == '긍정']\n",
    "neutral_indices = [idx for idx, label in enumerate(c_dataset['output']) if label == '중립']\n",
    "negative_indices = [idx for idx, label in enumerate(c_dataset['output']) if label == '부정']\n",
    "\n",
    "# 각 'output' 값에 대한 필요한 수의 샘플 선택\n",
    "positive_train_size = 1263\n",
    "neutral_train_size = 2296\n",
    "negative_train_size = 504\n",
    "\n",
    "positive_test_size = len(positive_indices) - positive_train_size\n",
    "neutral_test_size = len(neutral_indices) - neutral_train_size\n",
    "negative_test_size = len(negative_indices) - negative_train_size\n",
    "\n",
    "# 훈련 데이터 인덱스 선택\n",
    "positive_train_indices = positive_indices[:positive_train_size]\n",
    "neutral_train_indices = neutral_indices[:neutral_train_size]\n",
    "negative_train_indices = negative_indices[:negative_train_size]\n",
    "\n",
    "# 테스트 데이터 인덱스 선택\n",
    "positive_test_indices = positive_indices[positive_train_size:]\n",
    "neutral_test_indices = neutral_indices[neutral_train_size:]\n",
    "negative_test_indices = negative_indices[negative_train_size:]\n",
    "\n",
    "# 훈련 데이터셋 및 테스트 데이터셋 생성\n",
    "train_dataset_indices = positive_train_indices + neutral_train_indices + negative_train_indices\n",
    "test_dataset_indices = positive_test_indices + neutral_test_indices + negative_test_indices\n",
    "\n",
    "train_dataset = Dataset.from_dict({key: [c_dataset[key][idx] for idx in train_dataset_indices] for key in c_dataset.features})\n",
    "test_dataset = Dataset.from_dict({key: [c_dataset[key][idx] for idx in test_dataset_indices] for key in c_dataset.features})\n",
    "\n",
    "# 결과 확인\n",
    "print(\"Train set shape:\", len(train_dataset))\n",
    "print(\"Test set shape:\", len(test_dataset))\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"프로그램 실행 시간: \", elapsed_time, \"초\")\n",
    "\n",
    "c_dataset = train_dataset\n",
    "\n",
    "# 데이터셋의 크기\n",
    "total_samples = len(s_dataset)\n",
    "\n",
    "# 훈련 데이터의 크기\n",
    "train_size = 4492\n",
    "\n",
    "# 훈련 데이터 인덱스 범위 선택\n",
    "train_indices = list(range(train_size))\n",
    "\n",
    "# 테스트 데이터 인덱스 범위 선택 (나머지)\n",
    "test_indices = list(range(train_size, total_samples))\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터로 분할\n",
    "train_dataset = s_dataset.select(train_indices)\n",
    "test_dataset = s_dataset.select(test_indices)\n",
    "\n",
    "# 데이터셋 형태 확인\n",
    "print(\"Train set shape:\", len(train_dataset))\n",
    "print(\"Test set shape:\", len(test_dataset))\n",
    "\n",
    "s_dataset = train_dataset\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# 각 타입에 해당하는 인덱스를 구합니다.\n",
    "single_indices = [idx for idx, label in enumerate(chat_dataset['type']) if label == 'single']\n",
    "multi2_indices = [idx for idx, label in enumerate(chat_dataset['type']) if label == 'multi_2']\n",
    "multi3_indices = [idx for idx, label in enumerate(chat_dataset['type']) if label == 'multi_3']\n",
    "\n",
    "# 각 타입에 대해 조정할 샘플 수를 정의합니다.\n",
    "single_samples = 6094\n",
    "multi2_samples = 3712\n",
    "multi3_samples = 10756\n",
    "\n",
    "# 샘플 인덱스를 선택합니다.\n",
    "selected_indices = (single_indices[:single_samples] + \n",
    "                    multi2_indices[:multi2_samples] + \n",
    "                    multi3_indices[:multi3_samples])\n",
    "\n",
    "# 선택된 인덱스를 제외한 나머지를 테스트용으로 사용합니다.\n",
    "test_indices = set(range(len(chat_dataset))) - set(selected_indices)\n",
    "\n",
    "# train 및 test 데이터셋을 생성합니다.\n",
    "train_dataset = chat_dataset.select(selected_indices)\n",
    "test_dataset = chat_dataset.select(list(test_indices))\n",
    "\n",
    "# 결과 확인\n",
    "print(\"Train set shape:\", len(train_dataset))\n",
    "print(\"Test set shape:\", len(test_dataset))\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"프로그램 실행 시간: \", elapsed_time, \"초\")\n",
    "\n",
    "chat_dataset = train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58c5a679-b5b1-4d22-a99d-dcc09a331cbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5002cfc5b2774cab95d4987aa8749f4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14682 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "task_dataset = task_combined_dataset(c_dataset, s_dataset)\n",
    "task_dataset = DatasetDict({'train': task_dataset})\n",
    "task_dataset = process_dataset(task_dataset['train'])\n",
    "# 두 데이터셋 합치기\n",
    "combined_dataset = concatenate_datasets([chat_dataset,task_dataset])\n",
    "dataset = combined_dataset.shuffle(seed=42)  # seed 값은 원하는 값으로 변경 가능\n",
    "dataset = dataset.shuffle(seed=42)  # seed 값은 원하는 값으로 변경 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcbc48df-c8a9-4fe3-ad87-e7c0954c390e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['instruction', 'output', 'source', 'type', 'text'],\n",
      "    num_rows: 41344\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b82744e-f288-49bf-9341-334cb36b34d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff42f698-2430-4d8b-a243-7aece3b040a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# QLoRA parameters\n",
    "################################################################################\n",
    "\n",
    "# LoRA attention dimension\n",
    "lora_r = 16\n",
    "\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 16\n",
    "\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0 # 0.1\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34980d83-7132-49de-9384-7caff83affc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndevice_map =  {\\n    0: [0, 1, 2, 3, 4, 5, 6, 7],   # GPU 1에 8개 레이어 할당\\n    1: [8, 9, 10, 11, 12, 13, 14, 15],  # GPU 2에 다음 8개 레이어 할당\\n    2: [16, 17, 18, 19, 20, 21, 22, 23],  # GPU 3에 나머지 8개 레이어 할당\\n    3: [24, 25, 26, 27, 28, 29, 30, 31]\\n    }\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################################################################################\n",
    "# TrainingArguments parameters\n",
    "################################################################################\n",
    "\n",
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = \"./results\"\n",
    "\n",
    "# Number of training epochs\n",
    "num_train_epochs = 2\n",
    "\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = False\n",
    "bf16 = False\n",
    "\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 2\n",
    "\n",
    "# Batch size per GPU for evaluation\n",
    "per_device_eval_batch_size = 2\n",
    "\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 4 # 1\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "gradient_checkpointing = True\n",
    "\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm = 0.3\n",
    "\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 2e-4\n",
    "\n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "weight_decay = 0.01 # 0.001\n",
    "\n",
    "# Optimizer to use\n",
    "optim = 'paged_adamw_8bit'# \"paged_adamw_32bit\"\n",
    "\n",
    "# Learning rate schedule (constant a bit better than cosine)\n",
    "lr_scheduler_type =  'linear'#\"constant\"\n",
    "\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = -1\n",
    "\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "# Group sequences into batches with same length\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = True\n",
    "\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 37000\n",
    "\n",
    "# Log every X updates steps\n",
    "logging_steps = 2000\n",
    "\n",
    "################################################################################\n",
    "# SFT parameters\n",
    "################################################################################\n",
    "\n",
    "# Maximum sequence length to use\n",
    "max_seq_length = None # 2048\n",
    "\n",
    "# Pack multiple short examples in the same input sequence to increase efficiency\n",
    "packing = False\n",
    "\n",
    "# Load the entire model on the GPU 0\n",
    "#device_map = {\"\": 0}\n",
    "device_map = 'auto'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3aca536a-f27b-421c-92e2-604ef339c7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model with QLoRA configuration\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit, # True \n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)# Check GPU compatibility with bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b439b41-0558-48c6-aba8-1d9d12c18bd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47aeed6ea40f466796c69768ed9a57d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bbc96f30-b232-4bdf-8bc2-9752615930e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Load LLaMA tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e595ecda-2286-4eee-b103-84d9253f2c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 41,943,040 || all params: 8,072,204,288 || trainable%: 0.5195983464188562\n"
     ]
    }
   ],
   "source": [
    "#model.gradient_checkpointing_enable() # 훈련속도가 약간 감소하는 대신, 메모리 사용량을 크게 줄임, 속도 저하는 20~30 % 정도\n",
    "# 테스트 결과 해당 옵션 문제였음 크게 감소도 안하니 생략 \n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Load LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha, # 16\n",
    "    lora_dropout=lora_dropout, # 0\n",
    "    r=lora_r, # 16 # The number of LoRA layers 8, 16, 32, 64\n",
    "    #use_gradient_checkpointing = True, # Use gradient checkpointing\n",
    "    #use_rslora = False, # Use RSLora\n",
    "    #use_dora = False, # Use DoRa\n",
    "    #loftq_config = None, # The LoFTQ configuration\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules= [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\"], # The target modules\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "# trainable params: 33,554,432 || all params: 6,889,410,560 || trainable%: 0.4870435824338505"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dafebb80-9e13-4fe0-a141-056d52343841",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\any\\anaconda3\\envs\\oosij\\lib\\site-packages\\trl\\trainer\\sft_trainer.py:246: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d64a55a47684852aff242dd4418c484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/41344 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\any\\anaconda3\\envs\\oosij\\lib\\site-packages\\accelerate\\accelerator.py:443: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Set training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "# Set supervised fine-tuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=packing,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07e83703-3123-4f24-9b8d-3abb48017656",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "C:\\Users\\any\\anaconda3\\envs\\oosij\\lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10336' max='10336' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10336/10336 57:33:10, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.991400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.899800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.846400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.795000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.782900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10336, training_loss=0.8595301521820918, metrics={'train_runtime': 207235.7309, 'train_samples_per_second': 0.399, 'train_steps_per_second': 0.05, 'total_flos': 1.5255286391115448e+18, 'train_loss': 0.8595301521820918, 'epoch': 2.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7b3f45e-8531-4898-96e8-9f34b5784708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-980e98ce40f27e91\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-980e98ce40f27e91\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 4001;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorboard import notebook\n",
    "output_dir = \"./results\"\n",
    "\n",
    "log_dir = \"results/runs\"\n",
    "notebook.start(\"--logdir {} --port 4001\".format(log_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730a6a09-32c3-4d40-8743-0bd994112687",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "output_dir = \"./results\"\n",
    "\n",
    "# Ensure TensorBoard logs directory exists\n",
    "log_dir = os.path.join(output_dir, 'runs')\n",
    "\n",
    "# Start TensorBoard server\n",
    "os.system(f'tensorboard --logdir={log_dir} --port=6006')\n",
    "\n",
    "# Wait a bit for TensorBoard to start\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b667ceb4-5ed3-4d38-b63d-0b9e2bc92466",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb057afc-dcdb-4ce4-a458-2b9a591587b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): Linear4bit(\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "              )\n",
       "              (k_proj): Linear4bit(\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "              )\n",
       "              (v_proj): Linear4bit(\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "              )\n",
       "              (o_proj): Linear4bit(\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear4bit(\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              )\n",
       "              (up_proj): Linear4bit(\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              )\n",
       "              (down_proj): Linear4bit(\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=14336, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logging.set_verbosity(logging.CRITICAL)\n",
    "model.config.use_cache = True\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c9dee05-f53f-4c78-95ef-779ce61e784e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('models/Llama-3-Open-Ko-8B-Instruct-preview_train_v5_eval\\\\tokenizer_config.json',\n",
       " 'models/Llama-3-Open-Ko-8B-Instruct-preview_train_v5_eval\\\\special_tokens_map.json',\n",
       " 'models/Llama-3-Open-Ko-8B-Instruct-preview_train_v5_eval\\\\tokenizer.json')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_path_eval = 'models/' +  new_model + '_eval'\n",
    "\n",
    "# Save trained model / tokenizer\n",
    "trainer.model.save_pretrained(save_path_eval )\n",
    "trainer.tokenizer.save_pretrained(save_path_eval )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715a13e0-3ff9-49b7-8c17-a9c1526fbfaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3088e330-d4f9-41d1-b7a0-8e51a0a4f17c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179fc121-67b3-4aee-9dd0-a758ac22dc8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oosij",
   "language": "python",
   "name": "oosij"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
