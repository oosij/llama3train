{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "60c14951-587c-4dbe-a203-3c44fe6293e4",
      "metadata": {
        "jupyter": {
          "output_area": {
            "scrolling": true,
            "max_height": "1000px"
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bin C:\\Users\\any\\anaconda3\\envs\\oosij\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda118.dll\n"
          ]
        }
      ],
      "source": [
        "# https://medium.com/@miloszivic99/finetuning-large-language-models-customize-llama-3-8b-for-your-needs-bfe0f43cd239\n",
        "import re \n",
        "import os\n",
        "import os.path as osp\n",
        "import torch\n",
        "import pandas\n",
        "import pandas as pd\n",
        "import json\n",
        "from typing import Union\n",
        "from typing import List\n",
        "from datasets import load_dataset\n",
        "from datasets import Dataset, DatasetDict\n",
        "from datasets import concatenate_datasets\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        ")\n",
        "from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n",
        "from trl import SFTTrainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "9625fe1e-d3e0-4688-be9c-5e399f80d169",
      "metadata": {
        "jupyter": {
          "output_area": {
            "scrolling": true,
            "max_height": "1000px"
          }
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Llama-3-Open-Ko-8B-Instruct-preview_train_v5'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "c_path = './data/classification/ukairia777/finance_data.csv' # 분류 .csv\n",
        "s_path = './data/summarization/aihub_news' # 요약 폴더내.json 들\n",
        "template_path = './templates/multi.json' # 템플릿  .json\n",
        "dataset_name = \"Smoked-Salmon-s/empathetic_dialogues_ko\" # 싱글/멀티 대화형 데이터셋  허깅페이스 :\n",
        "\n",
        "model_name = \"beomi/Llama-3-Open-Ko-8B-Instruct-preview\"\n",
        "\n",
        "name = model_name.split('/')[1]\n",
        "new_model = name + \"_train_v5\" \n",
        "new_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "715a13e0-3ff9-49b7-8c17-a9c1526fbfaa",
      "metadata": {
        "jupyter": {
          "output_area": {
            "scrolling": true,
            "max_height": "1000px"
          }
        }
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4324634a-3789-438d-b31b-c4be4f729c31",
      "metadata": {
        "jupyter": {
          "output_area": {
            "scrolling": true,
            "max_height": "1000px"
          }
        }
      },
      "outputs": [],
      "source": [
        "## Model Load "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b6b00177-8928-4631-87e4-4dac276f1166",
      "metadata": {
        "jupyter": {
          "output_area": {
            "scrolling": true,
            "max_height": "1000px"
          }
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'models/Llama-3-Open-Ko-8B-Instruct-preview_train_v5_eval'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "save_path = 'models/' +  new_model + '_eval'\n",
        "save_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd4e3d86-f377-410d-921e-9252c2df24df",
      "metadata": {
        "jupyter": {
          "output_area": {
            "scrolling": true,
            "max_height": "1000px"
          }
        }
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6c0aec45-f99a-4a89-810d-8de4b232a54f",
      "metadata": {
        "jupyter": {
          "output_area": {
            "scrolling": true,
            "max_height": "1000px"
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c97ed07a0c1b4b24be19ac870e440221",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "# Reload model in FP16 and merge it with LoRA weights\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    low_cpu_mem_usage=True,\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map= 'auto',\n",
        ")\n",
        "model = PeftModel.from_pretrained(base_model, save_path)\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "# Reload tokenizer to save it\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0284e26-ab2d-4159-9ba1-804a0f91931f",
      "metadata": {
        "jupyter": {
          "output_area": {
            "scrolling": true,
            "max_height": "1000px"
          }
        }
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4deb2dd-7da7-4938-83e9-37499afd2dab",
      "metadata": {
        "jupyter": {
          "output_area": {
            "scrolling": true,
            "max_height": "1000px"
          }
        }
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "4db57ae7-c87b-421a-b7ba-28a41f2a8430",
      "metadata": {
        "jupyter": {
          "output_area": {
            "scrolling": true,
            "max_height": "1000px"
          }
        }
      },
      "outputs": [],
      "source": [
        "#  추론용\n",
        "def inference_output(prompt):\n",
        "    terminators = [\n",
        "    tokenizer.eos_token_id,\n",
        "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "    ]\n",
        "    input_ids = tokenizer.apply_chat_template(messages,add_generation_prompt=True,return_tensors=\"pt\").to(model.device)\n",
        "    # 인퍼런스 커스텀\n",
        "    #inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).to(\"cuda\") # return_token_type_ids=False\n",
        "    # peft 일시, **tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False) 아래 inputs['input_ids'] 교체 , 위는 삭제 \n",
        "    output = model.generate(\n",
        "        #**tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False),\n",
        "        input_ids,\n",
        "        max_new_tokens = 512, # 200  256 512\n",
        "        do_sample= True, \n",
        "        top_p= 0.9,\n",
        "        #top_k = 1, \n",
        "        temperature= 1, # 0.37,\n",
        "        #early_stopping= True,\n",
        "        eos_token_id=terminators,  \n",
        "    )\n",
        "    \n",
        "    output = output[0].to(\"cpu\")\n",
        "    outputs = tokenizer.decode(output)\n",
        "    return  outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "06487346-2a13-4c49-bb07-36304e9641ac",
      "metadata": {
        "jupyter": {
          "output_area": {
            "scrolling": true,
            "max_height": "1000px"
          }
        }
      },
      "outputs": [
        {
          "name": "stdin",
          "output_type": "stream",
          "text": [
            "[your msg] =  안녕하세요?\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Pixy msg] =  안녕하세요! 저는 AI 봇 픽시입니다. 당신의 일상에서 가장 기억에 남는 순간이 무엇인지 알려주실 수 있나요?\n"
          ]
        },
        {
          "name": "stdin",
          "output_type": "stream",
          "text": [
            "[your msg] =  지금 더운 것이 가장 기억에 남네요.  관련된 뉴스를 요약해주시겠어요?     \"목요일인 20일 중부 지방과 전북권, 경북권의 낮 기온이 33도 안팎으로 오르면서 덥겠다.     전남권과 경남권, 제주도는 비가 내리거나 구름이 짙게 끼면서 낮 기온이 전날보다 2∼6도가량 떨어지겠다.      전국 대부분 지역에서 체감온도가 최고 31도 이상 오르며 무더운 날씨를 보인 11일 서울 영등포구 여의대로에 지열로 인한 아지랑이가 피어오르고 있다. 뉴스1    이날 오전 5시 현재 기온은 서울 23.7도, 인천 22.0도, 수원 19.8도, 춘천 19.1도, 강릉 25.7도, 청주 22.9도, 대전 20.3도, 전주 20.8도, 광주 21.8도, 제주 22.2도, 대구 21.3도, 부산 21.6도, 울산 19.8도, 창원 20.2도 등이다.     낮 최고기온은 25∼35도로 예보됐다.     이날 전국에 가끔 구름이 많은 가운데 전남권과 제주도에 비가 내리겠다.     특히 제주도는 돌풍과 천둥·번개를 동반한 시간당 30㎜ 이상의 매우 강한 장맛비가 오겠다.  오전에 전북 남부와 경남권도 가끔 비가 오는 곳이 있겠으나 밤에 대부분 그치겠다.     예상 강수량은 전남 남해안 5∼20㎜, 경남 남해안 5∼10㎜, 광주·전남, 부산·울산·경남 내륙 5㎜ 안팎, 전북 남부 5㎜ 미만이다.     제주도는 이튿날 아침까지 50∼100㎜(많은 곳 150㎜ 이상, 산지 200㎜ 이상)의 비가 예보됐다.     미세먼지 농도는 전국이 ‘좋음’∼‘보통’ 수준으로 예상된다.     다만 수도권은 전날 잔류 미세먼지가 대기 정체와 기류 수렴으로 축적되며 오전에 잠시 ‘나쁨’ 수준을 보이겠다.\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Pixy msg] =  목요일인 20일 중부 지방과 전북권, 경북권의 낮 기온이 33도 안팎으로 오르면서 덥겠다. 전남권과 경남권, 제주도는 비가 내리거나 구름이 짙게 끼면서 낮 기온이 전날보다 2∼6도가량 떨어지겠다. 전국 대부분 지역에서 체감온도가 최고 31도 이상 오르며 무더운 날씨를 보인 11일 서울 영등포구 여의대로에 지열로 인한 아지랑이가 피어오르고 있다.\n"
          ]
        },
        {
          "name": "stdin",
          "output_type": "stream",
          "text": [
            "[your msg] =  좋습니다. 다음으로 그 요약한 것을 감정 분류해주세요. 라벨은 \"긍정\", \"부정\", \"중립\" 3가지 중 한가지로 대답해주세요.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Pixy msg] =  중립\n"
          ]
        },
        {
          "name": "stdin",
          "output_type": "stream",
          "text": [
            "[your msg] =  고맙습니다. 몇가지 대화를 해봅시다.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Pixy msg] =  네, 여러 대화를 통해 더욱 향상된 대화 능력을 개발하고 싶습니다. 다음 대화에서 어떤 주제로 이야기를 나누면 좋을까요?\n"
          ]
        },
        {
          "name": "stdin",
          "output_type": "stream",
          "text": [
            "[your msg] =  네 당신이 할 수 있는 일은 무엇이 있죠?\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Pixy msg] =  네, 저는 당신의 일상에서 불편한 점을 찾아보고 해결책을 제안하는 등의 일을 도와드릴 수 있습니다.\n"
          ]
        },
        {
          "name": "stdin",
          "output_type": "stream",
          "text": [
            "[your msg] =  좋습니다. 하지만 그 기회는 다음으로 미뤄야 할 것 같아요. 대화를 이만 종료합시다.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Pixy msg] =  알겠습니다. 다음에 만나서 더 즐거운 대화를 나누도록 하겠습니다. 그럼 이만.\n"
          ]
        },
        {
          "name": "stdin",
          "output_type": "stream",
          "text": [
            "[your msg] =  0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Pixy msg] =  대화를 종료합니다.\n"
          ]
        }
      ],
      "source": [
        "## 최종 챗\n",
        "import warnings\n",
        "# 경고 메시지를 무시하도록 설정\n",
        "warnings.filterwarnings(\"ignore\", message=\"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\")\n",
        "warnings.filterwarnings(\"ignore\", message=\"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\")\n",
        "# 트랜스포머 경고 메시지를 무시하도록 설정\n",
        "logging.set_verbosity_error()\n",
        "\n",
        "sys_msg = \"당신은 업무적으로 도움을 주고 공감 능력이 있는 AI 봇 픽시입니다. 다음 대화의 흐름을 보고 상대방의 요청에 최대한 자세하고 친절하게 답해주세요.\"\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\":sys_msg}\n",
        "]\n",
        "\n",
        "eos_token = '<|eot_id|>'\n",
        "\n",
        "split_answer = '<|eot_id|><|start_header_id|>assistant<|end_header_id|>'\n",
        "\n",
        "while True : \n",
        "    user_input = input('[your msg] = ')\n",
        "    if user_input == '0':\n",
        "        print('[Pixy msg] =  대화를 종료합니다.')\n",
        "        break\n",
        "    user_prompt = {\"role\": \"user\", \"content\": user_input}  \n",
        "    messages.append(user_prompt)\n",
        "    output = inference_output(messages)\n",
        "    response= output.split(split_answer)[-1].strip().replace(eos_token, '')\n",
        "    print('[Pixy msg] = ', response)\n",
        "    bot_output = {\"role\": \"assistant\", \"content\": response}\n",
        "    messages.append(bot_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "028ce601-48e5-4fe4-b0c0-c3bb928ce9a7",
      "metadata": {
        "jupyter": {
          "output_area": {
            "scrolling": true,
            "max_height": "1000px"
          }
        }
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5358dce-1d7c-46f7-adab-d23e28b4a35c",
      "metadata": {
        "jupyter": {
          "output_area": {
            "scrolling": true,
            "max_height": "1000px"
          }
        }
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06314cfc-c75f-4215-b0be-cf6b72396f04",
      "metadata": {
        "jupyter": {
          "output_area": {
            "scrolling": true,
            "max_height": "1000px"
          }
        }
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d624ad7b-303f-489d-a6eb-67de8e27507d",
      "metadata": {
        "jupyter": {
          "output_area": {
            "scrolling": true,
            "max_height": "1000px"
          }
        }
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "3088e330-d4f9-41d1-b7a0-8e51a0a4f17c",
      "metadata": {
        "jupyter": {
          "output_area": {
            "scrolling": true,
            "max_height": "1000px"
          }
        }
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "179fc121-67b3-4aee-9dd0-a758ac22dc8b",
      "metadata": {
        "jupyter": {
          "output_area": {
            "scrolling": true,
            "max_height": "1000px"
          }
        }
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "oosij",
      "language": "python",
      "name": "oosij"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}